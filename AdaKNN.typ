#import "template.typ": *

#show: apply-template.with(
  title:[*AdaBoost-KNN* *Project Report*],
  right_header:"数据建模与分析课程作业",
  left_header:"Statistical Learning HomeWork",
  author: "Maythics",
  ID:"unknown",
  )

\

=== Repository

#t 本次作业的notebook代码以及报告等相关内容均已经上传至我的仓库，如有疑问请访问#text(purple)[https://github.com/Maythics/AdaKNN-report]

= Introduction

== Method

#t 概述方法：Adaboost本来用于二分裂问题，但是KNN则是针对多分类（KNN二分类也没什么意思），因此需要知道如何让Adaboost支持多分类，下面是（详见reference）简单陈述：

对于多维，将样本的标签也设置为多维的，比如：$y = cases(1 quad y in k "family",-1/(k-1)  quad "else" )$

修改传统的Adaboost中的指数损失函数为：

$ L(y,f(x)) =sum_i exp(-1/k vectorarrow(y_i) dot vectorarrow(f_m(x_i))) $

把当前的分类器拆分成之前的加上最近一步的：
$ f_m(x)=f_(m-1)(x)+alpha_m g_m(x) $

#t 然后代入，看看损失函数在正确与否的情况下分别是多少，得到一个能用事情函数表达出来的式子，再对$alpha$求偏导，思路和李航书中Adaboost相同……

#t 最终，得到的算法如下：

+ 初始化权重$w_i$

+ 在权重$w_i$下训练分类器

+ 计算错误率$"error"= sum_i w_i I(g_m eq.not vectorarrow(y_i))/(sum_i w_i)$

+ 计算子分类器权重$alpha_m = ln((1-"error")/"error")+ln(N-1)$，$N$是类别数

+ 更新样本点权重$w_i=w_i exp(alpha_m I(g_m eq.not vectorarrow(y_i)))$

+ 归一化权重，然后循环往复

#t 下面还要解决一个问题，就是如何将KNN用于优化，因为KNN基于的是固定的k个最近邻，好像没什么参数可以调的，于是这里使用的是weigthed KNN，这样就有权重了

#t 具体含义

#t 例子：比如 $k=3$ ，则先找三个最邻近的点，它们权重分别为A(0.3)，A(0.6)，B(0.7)，0.3+0.6=0.9>0.7 因此，以最后选择归到A类中

#t 可以看出，其实这就是一个投票池系统，每来一个新的点，就先找k个最近，然后再发动这k个代表带权投票，投票值最大的类胜出；再来一个点，再发动一次找最近，最近的几个再带权投票……

#t 在本Project中，训练时采用的是随机滚动洗牌的方法，输入某些带有标签的数据后，这样训练：

+ 第一轮，先人为地从数据中分出一个测试集，剩余的当模型点集

+ 根据模型点集中的点，尝试用weighted KNN分类测试集

+ 根据在测试集上的表现，更新测试集的权重（按照上面的Adaboost）

+ 归一化权重，我这里是将训练集+模型点集一起归一化

+ 将模型集与训练集合并，然后洗牌，重新分出测试集和模型集，重复

#t 在某一轮分错的点，其权重会增大，之后的几轮中，它被洗牌到模型集之后，就会在投票时获得更大的票数。也就是，“之前的分错会导致之后投票时话语权更大”，这样就实现了weighted KNN的变权重，之后会更加照顾分错点的意见

== Reference

#t 我参考了两篇文献以及CSDN论坛

#t 一篇是#text(blue)[改进的AdaBoost集成学习方法研究[D].暨南大学,2021.DOI:10.27167/d.cnki.gjinu.2020.001350.]

#t 一篇是#text(blue)[Zhu, H. Zou, S. Rosset, T. Hastie, “Multi-class AdaBoost”, 2009.]

#t CSDN内容：#text(blue)[https://blog.csdn.net/weixin_43298886/article/details/110927084]

= Codes and Results

== experiment environment

#t 我使用Anaconda中的环境，使用jupyternotebook进行编辑，使用了*torch*库，其版本为2.2.1

#figure(
  image("./images/torch ver.png", width:50%),
  caption: "检查版本"
)

== Codes

代码如下：

#codex(read("./code/AdaKNN.py"),lang :"python")


== Explanation

代码解释：主要的函数是这几个：

+ KNN与knn，这个类是用来生成某一个knn的，比如：`knn = KNN(3)`就能创建一个找出3个最近点的函数knn，而knn函数的输入是模型点集和其标签和其权重（x_train, y_train,w_train），以及无标签的预测点集（x_test），返回的是一个张量pred_labels，存有个根据这个weighted knn得出的点集x_test预测出的标签结果

+ adjust_weight，该函数是用来更新权重，算出alpha之类的数据的。输入是之前预测点集的标签pred_labels，以及预测点集实际的标签y_test，模型点集目前的权重w_train，测试点集目前的权重w_test，还有本次分类的类别数N；输出是更新后的模型点集的权重w_train，更新后的测试点集权重w_test，以及本次更新对应的alpha

+ generate_test_and_train该函数用于洗牌，重新划分谁是模型点集，谁是测试点集，输入就是整合后的点集和其标签和其目前的权重（x_data以及y_data,w_data)

+ main 主函数，首先是输入数据x_data，y_data，w_data的给出，然后是产生一个knn函数。下面开始循环：每一次都先调用generate_test_and_train函数，根据x_data，y_data，w_data得到本轮的模型集和测试集，然后把它们输入knn中，得到测试集的预测结果，接着要根据预测的好坏调整权重，调用adjust_weight函数，得到更新后的权重。最后，把新权重、点集、标签全部再合并起来（torch.cat）得到新的x_data，y_data，w_data（和之前的区别就是权重已更新，而且顺序不同），开始下一轮

== Data set

#t 我采用了如下图所示的训练样本点(x_data与y_data)，其中不同颜色代表不同的类别，绿色代表0类，红色代表1类，蓝色代表2类

```python
x_data = torch.tensor([[1.0, -2.0], [-2.0, 3.0], [3.0, 4.0], [1.2, -0.5],[-3.2,4.5],[0.02, 3.5], [1.0, 2.0],[4,12],[10,-2],[3,14],[2.7,6],[5,1],[-4,2],[6,-2],[-0.5,2],[2,8],[3,0],[2,1],[-2,2],[-1.5,5],[2,4.7],[3.8,10],[0.5,3],[0.6,1.8],[2,3],[0.2,1.5],[-0.5,1],[-0.8,0.8],[-1,2.1]])

y_data = torch.tensor([0,2,1,0,2,  2,1,1,0,1,  1,0,2,0,2,1, 0,0,2,2,1, 1,2,1,1,1,1,1,2])
```

#figure(
  image("./images/dots.png", width:50%),
  caption: "代码中的data"
)

而测试样本点则是这10个点：

```python
testdata_x = torch.tensor([[1.0, -1.0], [-2.0, 1.2], [3.0, 10], [1.2, 0],[-1,2], [1.0, -1.5], [-1.7, 0.5], [3, 3], [1, 0.1],[-1,2.4]])

testdata_y = torch.tensor([0,2,1,0,2,0,1,1,0,2])
```

#t 设置本数据集原因在于：红色类是一个倾斜的狭窄的区域，插入到了蓝色和绿色两类中间，这给分类会带来一定挑战，使用普通的KNN算法很难处理红蓝绿交汇地带数据点的分类，如果经过Adaboost提升的KNN可以达到较好的分类效果，说明得到了一个较强的分类器，本方法有效。


== Results

`for cnt in range(0,5)`迭代5次时的输出：（这几个error tensor分别代表这几轮更新时的犯错率，打印出的一长串张量是我的模型，最后底部是最终模型的效果）

```python
error tensor(0.2500)
error tensor(0.7778)
error tensor(0.5000)
error tensor(0.2500)
error tensor(0.6667)

 Here is my classifier

x_train: tensor([[ 2.0000,  3.0000],
        [ 1.2000, -0.5000],
        [ 3.0000,  0.0000],
        [ 0.6000,  1.8000],
        [ 3.0000, 14.0000],
        [10.0000, -2.0000],
        [ 0.2000,  1.5000],
        [ 2.0000,  1.0000],
        [-2.0000,  3.0000],
        [ 2.0000,  8.0000],
        [ 5.0000,  1.0000],
        [ 1.0000, -2.0000],
        [-2.0000,  2.0000],
        [ 3.0000,  4.0000],
        [-1.5000,  5.0000],
        [-1.0000,  2.1000],
        [-3.2000,  4.5000],
        [ 1.0000,  2.0000],
        [ 0.5000,  3.0000],
        [-4.0000,  2.0000],
        [ 0.0200,  3.5000],
        [ 2.0000,  4.7000],
        [-0.5000,  1.0000],
        [ 3.8000, 10.0000],
        [ 4.0000, 12.0000]])
y_train: tensor([1, 0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1,
        1])
w_train: tensor([0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294,
        0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294,
        0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294, 0.0294])
alpha: 1.791759469228055
x_train: tensor([[ 0.2000,  1.5000],
        [10.0000, -2.0000],
        [ 2.0000,  3.0000],
        [ 2.7000,  6.0000],
        [-2.0000,  3.0000],
        [ 0.6000,  1.8000],
        [ 3.0000,  0.0000],
        [-0.5000,  1.0000],
        [ 3.8000, 10.0000],
        [-0.8000,  0.8000],
        [ 0.5000,  3.0000],
        [ 1.0000, -2.0000],
        [-1.5000,  5.0000],
        [ 3.0000,  4.0000],
        [-3.2000,  4.5000],
        [ 2.0000,  8.0000],
        [-1.0000,  2.1000],
        [ 1.2000, -0.5000],
        [ 4.0000, 12.0000],
        [-2.0000,  2.0000],
        [ 5.0000,  1.0000],
        [ 3.0000, 14.0000],
        [ 0.0200,  3.5000],
        [ 2.0000,  4.7000],
        [ 1.0000,  2.0000]])
y_train: tensor([1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 2, 0, 2, 1, 2, 1, 2, 0, 1, 2, 0, 1, 2, 1,
        1])
w_train: tensor([0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,
        0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,
        0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323])
alpha: -0.5596155346157144
x_train: tensor([[ 0.6000,  1.8000],
        [ 4.0000, 12.0000],
        [-0.8000,  0.8000],
        [ 0.0200,  3.5000],
        [-4.0000,  2.0000],
        [ 3.0000, 14.0000],
        [ 2.0000,  8.0000],
        [-1.5000,  5.0000],
        [ 2.0000,  4.7000],
        [ 3.0000,  0.0000],
        [ 1.0000, -2.0000],
        [ 2.0000,  1.0000],
        [-3.2000,  4.5000],
        [ 5.0000,  1.0000],
        [ 6.0000, -2.0000],
        [ 1.0000,  2.0000],
        [-1.0000,  2.1000],
        [ 3.8000, 10.0000],
        [-0.5000,  1.0000],
        [-2.0000,  2.0000],
        [10.0000, -2.0000],
        [ 3.0000,  4.0000],
        [ 2.0000,  3.0000],
        [-0.5000,  2.0000],
        [ 2.7000,  6.0000]])
y_train: tensor([1, 1, 1, 2, 2, 1, 1, 2, 1, 0, 0, 0, 2, 0, 0, 1, 2, 1, 1, 2, 0, 1, 1, 2,
        1])
w_train: tensor([0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,
        0.0303, 0.0303, 0.0173, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,
        0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.1039, 0.0303])
alpha: 0.6931471805599453
x_train: tensor([[-1.0000,  2.1000],
        [-4.0000,  2.0000],
        [-3.2000,  4.5000],
        [ 5.0000,  1.0000],
        [-2.0000,  2.0000],
        [ 3.0000,  4.0000],
        [ 2.0000,  3.0000],
        [ 2.0000,  4.7000],
        [ 2.0000,  8.0000],
        [ 2.0000,  1.0000],
        [ 3.0000, 14.0000],
        [ 1.2000, -0.5000],
        [ 0.5000,  3.0000],
        [-0.5000,  2.0000],
        [ 2.7000,  6.0000],
        [ 6.0000, -2.0000],
        [ 0.2000,  1.5000],
        [ 1.0000,  2.0000],
        [-0.8000,  0.8000],
        [ 1.0000, -2.0000],
        [ 0.0200,  3.5000],
        [ 3.8000, 10.0000],
        [ 3.0000,  0.0000],
        [-1.5000,  5.0000],
        [-0.5000,  1.0000]])
y_train: tensor([2, 2, 2, 0, 2, 1, 1, 1, 1, 0, 1, 0, 2, 2, 1, 0, 1, 1, 1, 0, 2, 1, 0, 2,
        1])
w_train: tensor([0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263,
        0.0150, 0.0263, 0.0263, 0.0526, 0.0902, 0.0263, 0.0263, 0.0526, 0.0263,
        0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263])
alpha: 1.791759469228055
x_train: tensor([[ 1.0000,  2.0000],
        [ 0.0200,  3.5000],
        [-1.0000,  2.1000],
        [ 6.0000, -2.0000],
        [ 0.2000,  1.5000],
        [ 0.5000,  3.0000],
        [-0.5000,  1.0000],
        [ 1.2000, -0.5000],
        [-1.5000,  5.0000],
        [-2.0000,  2.0000],
        [ 5.0000,  1.0000],
        [ 3.0000, 14.0000],
        [ 2.0000,  8.0000],
        [-3.2000,  4.5000],
        [10.0000, -2.0000],
        [-0.5000,  2.0000],
        [ 3.0000,  0.0000],
        [ 3.8000, 10.0000],
        [-4.0000,  2.0000],
        [-0.8000,  0.8000],
        [ 2.0000,  1.0000],
        [ 4.0000, 12.0000],
        [ 3.0000,  4.0000],
        [ 2.0000,  3.0000],
        [-2.0000,  3.0000]])
y_train: tensor([1, 2, 2, 0, 1, 2, 1, 0, 2, 2, 0, 1, 1, 2, 0, 2, 0, 1, 2, 1, 0, 1, 1, 1,
        2])
w_train: tensor([0.0263, 0.0263, 0.0263, 0.0263, 0.0526, 0.0526, 0.0263, 0.0263, 0.0263,
        0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0263, 0.0902, 0.0263, 0.0263,
        0.0263, 0.0263, 0.0150, 0.0263, 0.0263, 0.0263, 0.0263])
alpha: -5.960464655174746e-08

----------in the original data set, the prediction result is---------
2
error rate 0.06896551724137931
----------now lets test it with new input dots, the prediction result is---------
1
error rate 0.1
```
更改循环次数，cnt设置为`for cnt in range(0,10)`以及`for cnt in range(0,20)`

#figure(
  image("./images/cnt=10.png", width:70%),
  caption: "迭代10次后结果"
)

#figure(
  image("./images/result.png", width:70%),
  caption: "迭代20次后结果"
)

#t 可见，当迭代5次时，在原始样本集合上就有0.069的错误率，而在测试样本上有0.1的错误率；当迭代10次时，在原始样本上有0.0345的错误率，而在测试样本上全部分类正确；当迭代20次时，在原始训练集上达到完全正确分类，在测试样本上也完全正确，这说明本方法有效。

